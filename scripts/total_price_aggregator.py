#!/usr/bin/env python3
import json
import os
from datetime import datetime, timedelta
from collections import defaultdict
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TimestampTotalAggregator:
    def __init__(self, history_dir="data/price_history"):
        self.history_dir = history_dir
        self.intervals = ['1hour', '12hour', '1day']
        
        # ç·©å’Œç‰ˆè¨­å®š: ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’è¡¨ç¤º
        self.display_limits = {
            '1hour': 168,     # 1é€±é–“åˆ†å…¨ã¦è¡¨ç¤ºï¼ˆ48â†’168ï¼‰
            '12hour': 120,    # 2ãƒ¶æœˆåˆ†è¡¨ç¤ºï¼ˆ60â†’120ï¼‰
            '1day': 90        # 3ãƒ¶æœˆåˆ†è¡¨ç¤ºï¼ˆ30â†’90ï¼‰
        }
        
        # æ™‚é–“åŒºåˆ‡ã‚Šè¨­å®š
        self.bucket_intervals = {
            '1hour': timedelta(hours=1),      # 1æ™‚é–“åŒºåˆ‡ã‚Š
            '12hour': timedelta(hours=12),    # 12æ™‚é–“åŒºåˆ‡ã‚Š
            '1day': timedelta(days=1)         # 1æ—¥åŒºåˆ‡ã‚Š
        }
        
        # ç·©å’Œç‰ˆè¨­å®š
        self.force_aggregation = os.getenv('FORCE_AGGREGATION', 'true').lower() == 'true'
        self.include_zero_prices = os.getenv('INCLUDE_ZERO_PRICES', 'false').lower() == 'true'
        self.min_items_threshold = int(os.getenv('MIN_ITEMS_THRESHOLD', '1'))  # æœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°
        
        logger.info("ğŸ”§ ç·ä¾¡æ ¼é›†è¨ˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆç·©å’Œç‰ˆï¼‰åˆæœŸåŒ–")
        logger.info(f"ğŸ”„ å¼·åˆ¶é›†è¨ˆ: {'æœ‰åŠ¹' if self.force_aggregation else 'ç„¡åŠ¹'}")
        logger.info(f"ğŸ’° ã‚¼ãƒ­ä¾¡æ ¼å«ã‚€: {'æœ‰åŠ¹' if self.include_zero_prices else 'ç„¡åŠ¹'}")
        logger.info(f"ğŸ“Š æœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°: {self.min_items_threshold}")
        
    def safe_parse_price(self, price_value):
        """ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«è§£æï¼ˆç·©å’Œç‰ˆï¼‰"""
        try:
            if isinstance(price_value, (int, float)):
                price = int(price_value)
                # ç·©å’Œç‰ˆ: ã‚¼ãƒ­ä¾¡æ ¼ã‚‚æœ‰åŠ¹ã¨ã™ã‚‹è¨­å®š
                return price if (price >= 0 and self.include_zero_prices) or price > 0 else 0
            
            if isinstance(price_value, str):
                clean_price = price_value.replace(',', '').replace(' NESO', '').strip()
                if clean_price in ['', 'æœªå–å¾—', 'undefined', 'None', 'null']:
                    return 0
                
                price = int(float(clean_price))
                return price if (price >= 0 and self.include_zero_prices) or price > 0 else 0
            
            return 0
            
        except (ValueError, TypeError, AttributeError):
            return 0
    
    def round_to_bucket(self, timestamp_str, interval):
        """é–“éš”ã«å¿œã˜ãŸæ™‚é–“åŒºåˆ‡ã‚Šã«ä¸¸ã‚ã‚‹"""
        try:
            ts = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            bucket_interval = self.bucket_intervals[interval]
            
            if interval == '1hour':
                # 1æ™‚é–“åŒºåˆ‡ã‚Š
                bucket_start = ts.replace(minute=0, second=0, microsecond=0)
            elif interval == '12hour':
                # 12æ™‚é–“åŒºåˆ‡ã‚Šï¼ˆ0æ™‚ã¾ãŸã¯12æ™‚ï¼‰
                hour_bucket = (ts.hour // 12) * 12
                bucket_start = ts.replace(hour=hour_bucket, minute=0, second=0, microsecond=0)
            else:  # '1day'
                # 1æ—¥åŒºåˆ‡ã‚Šï¼ˆ0æ™‚ï¼‰
                bucket_start = ts.replace(hour=0, minute=0, second=0, microsecond=0)
            
            return bucket_start.isoformat()
        except Exception as e:
            logger.warn(f"æ™‚é–“ä¸¸ã‚ã‚¨ãƒ©ãƒ¼: {timestamp_str} -> {e}")
            return timestamp_str
    
    def aggregate_prices_per_bucket(self, item_data_points, interval):
        """æ™‚é–“åŒºåˆ‡ã‚Šã«å¿œã˜ã¦ä¾¡æ ¼ã‚’å¹³å‡åŒ–ï¼ˆç·©å’Œç‰ˆï¼‰"""
        # æ™‚é–“åŒºåˆ‡ã‚Šã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
        bucketed = defaultdict(list)
        processed_count = 0
        valid_count = 0
        
        for point in item_data_points:
            processed_count += 1
            
            if not isinstance(point, dict) or 'timestamp' not in point or 'price' not in point:
                continue
            
            bucket_time = self.round_to_bucket(point['timestamp'], interval)
            price = self.safe_parse_price(point['price'])
            
            # ç·©å’Œç‰ˆ: ã‚¼ãƒ­ä¾¡æ ¼ã‚‚æ¡ä»¶æ¬¡ç¬¬ã§å«ã‚ã‚‹
            if price > 0 or (price == 0 and self.include_zero_prices):
                bucketed[bucket_time].append(price)
                valid_count += 1
        
        # å„æ™‚é–“åŒºåˆ‡ã‚Šã®å¹³å‡ä¾¡æ ¼ã‚’è¨ˆç®—
        averaged_data = []
        for bucket_time in sorted(bucketed.keys()):
            prices = bucketed[bucket_time]
            if prices:
                avg_price = sum(prices) // len(prices)  # æ•´æ•°å¹³å‡
                averaged_data.append({
                    'timestamp': bucket_time,
                    'price': avg_price,
                    'original_count': len(prices)
                })
        
        return averaged_data
    
    def limit_data_points(self, data_points, interval):
        """è¡¨ç¤ºæœ€é©åŒ–ã®ãŸã‚ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’åˆ¶é™ï¼ˆç·©å’Œç‰ˆï¼‰"""
        limit = self.display_limits[interval]
        
        if len(data_points) <= limit:
            return data_points
        
        # æœ€æ–°ã®Nå€‹ã‚’å–å¾—
        limited_data = data_points[-limit:]
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆåˆ¶é™ {interval}: {len(data_points)} -> {len(limited_data)}ãƒã‚¤ãƒ³ãƒˆ")
        
        return limited_data
    
    def aggregate_by_timestamp(self, interval):
        """æŒ‡å®šé–“éš”ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚é–“åŒºåˆ‡ã‚Šã§é›†è¨ˆï¼ˆç·©å’Œç‰ˆï¼‰"""
        history_file = os.path.join(self.history_dir, f"history_{interval}.json")
        
        if not os.path.exists(history_file):
            logger.warn(f"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {history_file}")
            return []
        
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                all_items_data = json.load(f)
            
            if not isinstance(all_items_data, dict):
                logger.error(f"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒè¾æ›¸å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(all_items_data)}")
                return []
            
            logger.info(f"å±¥æ­´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ {interval}: {len(all_items_data)}ã‚¢ã‚¤ãƒ†ãƒ ")
            
            # æ™‚é–“åŒºåˆ‡ã‚Šã®æ™‚é–“æ åˆ¥ã«ç·ä¾¡æ ¼ã‚’é›†è¨ˆ
            timestamp_totals = defaultdict(lambda: {
                'total_price': 0, 
                'item_count': 0, 
                'total_points': 0,
                'valid_prices': [],
                'zero_prices': 0
            })
            
            processed_items = 0
            total_averaged_points = 0
            error_items = 0
            
            for item_id, item_history in all_items_data.items():
                if not isinstance(item_history, list):
                    error_items += 1
                    continue
                
                processed_items += 1
                
                # ã‚¢ã‚¤ãƒ†ãƒ æ¯ã«æ™‚é–“åŒºåˆ‡ã‚Šã§å¹³å‡åŒ–
                try:
                    averaged_points = self.aggregate_prices_per_bucket(item_history, interval)
                    total_averaged_points += len(averaged_points)
                    
                    # å¹³å‡åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒ³ãƒˆã‚’æ™‚é–“è»¸ã§åˆè¨ˆ
                    for avg_point in averaged_points:
                        bucket_time = avg_point['timestamp']
                        avg_price = avg_point['price']
                        original_count = avg_point['original_count']
                        
                        timestamp_totals[bucket_time]['total_price'] += avg_price
                        timestamp_totals[bucket_time]['item_count'] += 1
                        timestamp_totals[bucket_time]['total_points'] += original_count
                        timestamp_totals[bucket_time]['valid_prices'].append(avg_price)
                        
                        if avg_price == 0:
                            timestamp_totals[bucket_time]['zero_prices'] += 1
                
                except Exception as e:
                    error_items += 1
                    logger.debug(f"ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼ {item_id}: {e}")
            
            logger.info(f"ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†çµæœ: æˆåŠŸ{processed_items}ä»¶ã€ã‚¨ãƒ©ãƒ¼{error_items}ä»¶")
            
            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
            total_price_history = []
            for bucket_time in sorted(timestamp_totals.keys()):
                data = timestamp_totals[bucket_time]
                
                # ç·©å’Œç‰ˆ: æœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°ã®é–¾å€¤ã‚’ç·©å’Œ
                if data['item_count'] >= self.min_items_threshold:
                    total_price_history.append({
                        'timestamp': bucket_time,
                        'total_price': data['total_price'],
                        'item_count': data['item_count'],
                        'average_price': data['total_price'] // data['item_count'],
                        'original_points': data['total_points']
                    })
            
            # è¡¨ç¤ºæœ€é©åŒ–ã®ãŸã‚ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’åˆ¶é™
            limited_history = self.limit_data_points(total_price_history, interval)
            
            logger.info(f"æ™‚é–“åŒºåˆ‡ã‚Šé›†è¨ˆå®Œäº† {interval}: {len(limited_history)}æ™‚é–“æ ï¼ˆè¡¨ç¤ºæœ€é©åŒ–æ¸ˆã¿ï¼‰")
            logger.info(f"  å‡¦ç†ã‚¢ã‚¤ãƒ†ãƒ : {processed_items}, åŒºåˆ‡ã‚Šé–“éš”: {self.bucket_intervals[interval]}")
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
            if limited_history:
                prices = [p['total_price'] for p in limited_history]
                item_counts = [p['item_count'] for p in limited_history]
                logger.info(f"  ä¾¡æ ¼ç¯„å›²: {min(prices):,} - {max(prices):,} NESO")
                logger.info(f"  ã‚¢ã‚¤ãƒ†ãƒ æ•°ç¯„å›²: {min(item_counts)} - {max(item_counts)}ä»¶")
                
                # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã®è¨ˆç®—
                first_time = limited_history[0]['timestamp']
                last_time = limited_history[-1]['timestamp']
                try:
                    first_dt = datetime.fromisoformat(first_time.replace('Z', '+00:00'))
                    last_dt = datetime.fromisoformat(last_time.replace('Z', '+00:00'))
                    span = last_dt - first_dt
                    logger.info(f"  ãƒ‡ãƒ¼ã‚¿æœŸé–“: {span.days}æ—¥{span.seconds//3600}æ™‚é–“")
                except Exception as e:
                    logger.debug(f"æœŸé–“è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            
            return limited_history
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {interval}: {e}")
            return []
        except Exception as e:
            logger.error(f"é›†è¨ˆã‚¨ãƒ©ãƒ¼ {interval}: {e}")
            return []
    
    def generate_chart_data(self, interval, total_data):
        """Chart.jsç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆç·©å’Œç‰ˆï¼‰"""
        if not total_data:
            logger.error(f"ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãªã—: {interval}")
            return None
        
        try:
            def format_time(timestamp_str):
                try:
                    timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    if interval == '1hour':
                        return timestamp.strftime('%m/%d %H:00')
                    elif interval == '12hour':
                        return timestamp.strftime('%m/%d %H:00')
                    else:  # 1day
                        return timestamp.strftime('%m/%d')
                except Exception as e:
                    logger.warn(f"æ™‚åˆ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {timestamp_str} -> {e}")
                    return str(timestamp_str)
            
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆç·©å’Œç‰ˆï¼‰
            valid_points = []
            for point in total_data:
                if (isinstance(point, dict) and 
                    'timestamp' in point and 
                    'total_price' in point and 
                    'average_price' in point and
                    isinstance(point['total_price'], int) and
                    isinstance(point['average_price'], int)):
                    
                    # ç·©å’Œç‰ˆ: ã‚¼ãƒ­ä¾¡æ ¼ã‚‚æ¡ä»¶æ¬¡ç¬¬ã§æœ‰åŠ¹
                    if point['total_price'] > 0 or (point['total_price'] == 0 and self.include_zero_prices):
                        valid_points.append(point)
            
            if len(valid_points) == 0:
                logger.error(f"æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“: {interval}")
                return None
            
            labels = [format_time(point['timestamp']) for point in valid_points]
            total_prices = [point['total_price'] for point in valid_points]
            average_prices = [point['average_price'] for point in valid_points]
            
            # é–“éš”ã®èª¬æ˜
            interval_descriptions = {
                '1hour': '1é€±é–“åˆ†ï¼ˆ1æ™‚é–“æ¯ï¼‰',
                '12hour': '1ãƒ¶æœˆåˆ†ï¼ˆ12æ™‚é–“æ¯ï¼‰',
                '1day': '1å¹´åˆ†ï¼ˆ1æ—¥æ¯ï¼‰'
            }
            
            chart_data = {
                'labels': labels,
                'datasets': [
                    {
                        'label': f'ç·ä¾¡æ ¼ ({interval_descriptions.get(interval, interval)})',
                        'data': total_prices,
                        'borderColor': '#e74c3c',
                        'backgroundColor': 'rgba(231, 76, 60, 0.1)',
                        'borderWidth': 2,
                        'fill': True,
                        'tension': 0.4,
                        'pointRadius': 2,
                        'pointHoverRadius': 4,
                        'yAxisID': 'y'
                    },
                    {
                        'label': f'å¹³å‡ä¾¡æ ¼ ({interval_descriptions.get(interval, interval)})',
                        'data': average_prices,
                        'borderColor': '#3498db',
                        'backgroundColor': 'rgba(52, 152, 219, 0.1)',
                        'borderWidth': 2,
                        'fill': False,
                        'tension': 0.4,
                        'pointRadius': 1,
                        'pointHoverRadius': 3,
                        'yAxisID': 'y1'
                    }
                ]
            }
            
            logger.info(f"ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº† {interval}: {len(valid_points)}ãƒã‚¤ãƒ³ãƒˆ")
            logger.info(f"  æ™‚é–“ç¯„å›²: {labels[0]} ï½ {labels[-1]}")
            
            return chart_data
            
        except Exception as e:
            logger.error(f"ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼ {interval}: {e}")
            return None
    
    def save_total_data(self, interval, total_data):
        """ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆç·©å’Œç‰ˆï¼‰"""
        if not total_data:
            logger.warn(f"ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“: {interval}")
            return False
        
        try:
            # ç”Ÿãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ï¼ˆè©³ç´°æƒ…å ±ä¿æŒï¼‰
            raw_file = os.path.join(self.history_dir, f"total_price_optimized_{interval}.json")
            with open(raw_file, 'w', encoding='utf-8') as f:
                json.dump(total_data, f, ensure_ascii=False, indent=2)
            
            # Chart.jsç”¨ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ï¼ˆå¾“æ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«åç¶­æŒï¼‰
            chart_data = self.generate_chart_data(interval, total_data)
            if chart_data:
                chart_file = os.path.join(self.history_dir, f"total_price_{interval}.json")
                with open(chart_file, 'w', encoding='utf-8') as f:
                    json.dump(chart_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ä¿å­˜å®Œäº† {interval}: {len(total_data)}æ™‚é–“æ  -> {len(chart_data['labels'])}ãƒãƒ£ãƒ¼ãƒˆãƒã‚¤ãƒ³ãƒˆ")
            else:
                logger.error(f"ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå¤±æ•—: {interval}")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼ {interval}: {e}")
            return False
    
    def process_all(self):
        """å…¨é–“éš”ã®å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆç·©å’Œç‰ˆï¼‰"""
        results = {}
        
        logger.info("ğŸš€ ç·©å’Œç‰ˆé›†è¨ˆå‡¦ç†é–‹å§‹")
        
        for interval in self.intervals:
            logger.info(f"å‡¦ç†é–‹å§‹: {interval}")
            
            # æ™‚é–“åŒºåˆ‡ã‚Šã§é›†è¨ˆï¼ˆç·©å’Œç‰ˆï¼‰
            total_data = self.aggregate_by_timestamp(interval)
            
            if total_data:
                # ä¿å­˜
                saved = self.save_total_data(interval, total_data)
                
                results[interval] = {
                    'total_buckets': len(total_data),
                    'saved': saved,
                    'latest_total': total_data[-1]['total_price'] if total_data else 0,
                    'latest_count': total_data[-1]['item_count'] if total_data else 0,
                    'bucket_interval': str(self.bucket_intervals[interval]),
                    'display_limit': self.display_limits[interval]
                }
                
                # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
                if total_data:
                    prices = [p['total_price'] for p in total_data]
                    logger.info(f"  ä¾¡æ ¼ç¯„å›²: {min(prices):,} - {max(prices):,} NESO")
            else:
                results[interval] = {
                    'total_buckets': 0,
                    'saved': False,
                    'latest_total': 0,
                    'latest_count': 0,
                    'bucket_interval': str(self.bucket_intervals[interval]),
                    'display_limit': self.display_limits[interval]
                }
        
        return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼šç·©å’Œç‰ˆç·ä¾¡æ ¼é›†è¨ˆ"""
    logger.info("=" * 50)
    logger.info("MapleStoryç·ä¾¡æ ¼é›†è¨ˆï¼ˆç·©å’Œç‰ˆï¼‰é–‹å§‹")
    logger.info("=" * 50)
    
    try:
        aggregator = TimestampTotalAggregator()
        
        # é›†è¨ˆå‡¦ç†å®Ÿè¡Œ
        results = aggregator.process_all()
        
        # çµæœè¡¨ç¤º
        logger.info("ğŸ“Š ç·©å’Œç‰ˆé›†è¨ˆçµæœ:")
        for interval, result in results.items():
            logger.info(f"  {interval}: {result['total_buckets']}æ™‚é–“æ ")
            logger.info(f"    åŒºåˆ‡ã‚Šé–“éš”: {result['bucket_interval']}")
            logger.info(f"    è¡¨ç¤ºåˆ¶é™: {result['display_limit']}ãƒã‚¤ãƒ³ãƒˆ")
            if result['latest_total'] > 0:
                logger.info(f"    æœ€æ–°ç·ä¾¡æ ¼: {result['latest_total']:,} NESO ({result['latest_count']}ã‚¢ã‚¤ãƒ†ãƒ )")
        
        successful = sum(1 for r in results.values() if r['saved'])
        logger.info(f"âœ… ç·©å’Œç‰ˆå‡¦ç†å®Œäº†: {successful}/{len(aggregator.intervals)}é–“éš”æˆåŠŸ")
        
        return successful
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

if __name__ == "__main__":
    main()
