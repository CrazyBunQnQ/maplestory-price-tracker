#!/usr/bin/env python3
import json
import os
from datetime import datetime, timedelta
from collections import defaultdict
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TimestampTotalAggregator:
    def __init__(self, history_dir="data/price_history"):
        self.history_dir = history_dir
        self.intervals = ['1hour', '12hour', '1day']
        
    def safe_parse_price(self, price_value):
        """ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«è§£æ"""
        try:
            if isinstance(price_value, (int, float)):
                return int(price_value) if price_value > 0 else 0
            
            if isinstance(price_value, str):
                clean_price = price_value.replace(',', '').replace(' NESO', '').strip()
                if clean_price in ['', 'æœªå–å¾—', 'undefined', 'None', 'null']:
                    return 0
                return int(float(clean_price)) if float(clean_price) > 0 else 0
            
            return 0
            
        except (ValueError, TypeError, AttributeError):
            return 0
    
    def round_to_30min_bucket(self, timestamp_str):
        """30åˆ†åŒºåˆ‡ã‚Šã®æ™‚é–“æ ã«ä¸¸ã‚ã‚‹ï¼ˆæ¤œç´¢çµæœ[1]ã®æ‰‹æ³•ï¼‰"""
        try:
            ts = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            # 30åˆ†å˜ä½ã®æ™‚é–“æ ã«ä¸¸ã‚ã‚‹
            bucket_start = ts.replace(
                minute=(ts.minute // 30) * 30,
                second=0,
                microsecond=0
            )
            return bucket_start.isoformat()
        except Exception as e:
            logger.warn(f"æ™‚é–“ä¸¸ã‚ã‚¨ãƒ©ãƒ¼: {timestamp_str} -> {e}")
            return timestamp_str
    
    def aggregate_prices_per_30min(self, item_data_points):
        """30åˆ†æ¯ã«åŒºåˆ‡ã‚Šã€åŒã˜æ™‚é–“æ å†…ã®ä¾¡æ ¼ã‚’å¹³å‡åŒ–ï¼ˆæ¤œç´¢çµæœ[1]ãƒ™ãƒ¼ã‚¹ï¼‰"""
        # 30åˆ†åŒºåˆ‡ã‚Šã®æ™‚é–“æ ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
        bucketed = defaultdict(list)
        
        for point in item_data_points:
            if not isinstance(point, dict) or 'timestamp' not in point or 'price' not in point:
                continue
            
            bucket_time = self.round_to_30min_bucket(point['timestamp'])
            price = self.safe_parse_price(point['price'])
            
            if price > 0:
                bucketed[bucket_time].append(price)
        
        # å„æ™‚é–“æ ã®å¹³å‡ä¾¡æ ¼ã‚’è¨ˆç®—
        averaged_data = []
        for bucket_time in sorted(bucketed.keys()):
            prices = bucketed[bucket_time]
            if prices:
                avg_price = sum(prices) // len(prices)  # æ•´æ•°å¹³å‡
                averaged_data.append({
                    'timestamp': bucket_time,
                    'price': avg_price,
                    'original_count': len(prices)
                })
        
        return averaged_data
    
    def aggregate_by_timestamp(self, interval):
        """æŒ‡å®šé–“éš”ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’30åˆ†åŒºåˆ‡ã‚Šã§æ™‚é–“è»¸é›†è¨ˆ"""
        history_file = os.path.join(self.history_dir, f"history_{interval}.json")
        
        if not os.path.exists(history_file):
            logger.warn(f"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {history_file}")
            return []
        
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                all_items_data = json.load(f)
            
            if not isinstance(all_items_data, dict):
                logger.error(f"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒè¾æ›¸å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(all_items_data)}")
                return []
            
            # 30åˆ†åŒºåˆ‡ã‚Šã®æ™‚é–“æ åˆ¥ã«ç·ä¾¡æ ¼ã‚’é›†è¨ˆ
            timestamp_totals = defaultdict(lambda: {'total_price': 0, 'item_count': 0, 'total_points': 0})
            
            processed_items = 0
            total_averaged_points = 0
            
            for item_id, item_history in all_items_data.items():
                if not isinstance(item_history, list):
                    continue
                
                processed_items += 1
                
                # ã‚¢ã‚¤ãƒ†ãƒ æ¯ã«30åˆ†åŒºåˆ‡ã‚Šã§å¹³å‡åŒ–
                averaged_points = self.aggregate_prices_per_30min(item_history)
                total_averaged_points += len(averaged_points)
                
                # å¹³å‡åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒ³ãƒˆã‚’æ™‚é–“è»¸ã§åˆè¨ˆ
                for avg_point in averaged_points:
                    bucket_time = avg_point['timestamp']
                    avg_price = avg_point['price']
                    original_count = avg_point['original_count']
                    
                    timestamp_totals[bucket_time]['total_price'] += avg_price
                    timestamp_totals[bucket_time]['item_count'] += 1
                    timestamp_totals[bucket_time]['total_points'] += original_count
            
            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
            total_price_history = []
            for bucket_time in sorted(timestamp_totals.keys()):
                data = timestamp_totals[bucket_time]
                
                if data['item_count'] > 0:
                    total_price_history.append({
                        'timestamp': bucket_time,
                        'total_price': data['total_price'],
                        'item_count': data['item_count'],
                        'average_price': data['total_price'] // data['item_count'],
                        'original_points': data['total_points']  # ãƒ‡ãƒãƒƒã‚°ç”¨
                    })
            
            logger.info(f"30åˆ†åŒºåˆ‡ã‚Šé›†è¨ˆå®Œäº† {interval}: {len(total_price_history)}æ™‚é–“æ ")
            logger.info(f"  å‡¦ç†ã‚¢ã‚¤ãƒ†ãƒ : {processed_items}, å¹³å‡åŒ–å¾Œãƒã‚¤ãƒ³ãƒˆ: {total_averaged_points}")
            
            return total_price_history
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {interval}: {e}")
            return []
        except Exception as e:
            logger.error(f"é›†è¨ˆã‚¨ãƒ©ãƒ¼ {interval}: {e}")
            return []
    
    def generate_chart_data(self, interval, total_data):
        """Chart.jsç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆ30åˆ†åŒºåˆ‡ã‚Šå¯¾å¿œï¼‰"""
        if not total_data:
            logger.error(f"ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãªã—: {interval}")
            return None
        
        # 30åˆ†åŒºåˆ‡ã‚Šãªã®ã§å¿…ãšé€£ç¶šã—ãŸãƒ‡ãƒ¼ã‚¿ã«ãªã‚‹
        try:
            def format_time(timestamp_str):
                try:
                    timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    if interval == '1hour':
                        return timestamp.strftime('%m/%d %H:%M')
                    elif interval == '12hour':
                        return timestamp.strftime('%m/%d %H:%M')
                    else:  # 1day
                        return timestamp.strftime('%m/%d')
                except Exception as e:
                    logger.warn(f"æ™‚åˆ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {timestamp_str} -> {e}")
                    return str(timestamp_str)
            
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            valid_points = []
            for point in total_data:
                if (isinstance(point, dict) and 
                    'timestamp' in point and 
                    'total_price' in point and 
                    'average_price' in point and
                    isinstance(point['total_price'], int) and
                    isinstance(point['average_price'], int) and
                    point['total_price'] > 0):
                    valid_points.append(point)
            
            if len(valid_points) == 0:
                logger.error(f"æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“: {interval}")
                return None
            
            labels = [format_time(point['timestamp']) for point in valid_points]
            total_prices = [point['total_price'] for point in valid_points]
            average_prices = [point['average_price'] for point in valid_points]
            
            chart_data = {
                'labels': labels,
                'datasets': [
                    {
                        'label': f'ç·ä¾¡æ ¼ ({interval}) - 30åˆ†åŒºåˆ‡ã‚Šå¹³å‡',
                        'data': total_prices,
                        'borderColor': '#e74c3c',
                        'backgroundColor': 'rgba(231, 76, 60, 0.1)',
                        'borderWidth': 3,
                        'fill': True,
                        'tension': 0.3,
                        'pointRadius': 4,
                        'pointHoverRadius': 6,
                        'yAxisID': 'y'
                    },
                    {
                        'label': f'å¹³å‡ä¾¡æ ¼ ({interval}) - 30åˆ†åŒºåˆ‡ã‚Šå¹³å‡',
                        'data': average_prices,
                        'borderColor': '#3498db',
                        'backgroundColor': 'rgba(52, 152, 219, 0.1)',
                        'borderWidth': 2,
                        'fill': False,
                        'tension': 0.3,
                        'pointRadius': 3,
                        'pointHoverRadius': 5,
                        'yAxisID': 'y1'
                    }
                ]
            }
            
            logger.info(f"30åˆ†åŒºåˆ‡ã‚Šãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº† {interval}: {len(valid_points)}ãƒã‚¤ãƒ³ãƒˆ")
            logger.info(f"  æ™‚é–“ç¯„å›²: {labels[0]} ï½ {labels[-1]}")
            
            return chart_data
            
        except Exception as e:
            logger.error(f"ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼ {interval}: {e}")
            return None
    
    def save_total_data(self, interval, total_data):
        """ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆ30åˆ†åŒºåˆ‡ã‚Šå¯¾å¿œï¼‰"""
        if not total_data:
            logger.warn(f"ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“: {interval}")
            return False
        
        try:
            # ç”Ÿãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
            raw_file = os.path.join(self.history_dir, f"total_price_30min_{interval}.json")
            with open(raw_file, 'w', encoding='utf-8') as f:
                json.dump(total_data, f, ensure_ascii=False, indent=2)
            
            # Chart.jsç”¨ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
            chart_data = self.generate_chart_data(interval, total_data)
            if chart_data:
                chart_file = os.path.join(self.history_dir, f"total_price_{interval}.json")
                with open(chart_file, 'w', encoding='utf-8') as f:
                    json.dump(chart_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"30åˆ†åŒºåˆ‡ã‚Šä¿å­˜å®Œäº† {interval}: {len(total_data)}æ™‚é–“æ  -> {len(chart_data['labels'])}ãƒãƒ£ãƒ¼ãƒˆãƒã‚¤ãƒ³ãƒˆ")
            else:
                logger.error(f"ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå¤±æ•—: {interval}")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼ {interval}: {e}")
            return False
    
    def process_all(self):
        """å…¨é–“éš”ã®å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆ30åˆ†åŒºåˆ‡ã‚Šå¯¾å¿œï¼‰"""
        results = {}
        
        for interval in self.intervals:
            logger.info(f"30åˆ†åŒºåˆ‡ã‚Šå‡¦ç†é–‹å§‹: {interval}")
            
            # 30åˆ†åŒºåˆ‡ã‚Šã§æ™‚é–“è»¸é›†è¨ˆ
            total_data = self.aggregate_by_timestamp(interval)
            
            if total_data:
                # ä¿å­˜
                saved = self.save_total_data(interval, total_data)
                
                results[interval] = {
                    'total_buckets': len(total_data),
                    'saved': saved,
                    'latest_total': total_data[-1]['total_price'] if total_data else 0,
                    'latest_count': total_data[-1]['item_count'] if total_data else 0,
                    'time_range': f"{total_data[0]['timestamp']} ï½ {total_data[-1]['timestamp']}" if total_data else "N/A"
                }
                
                # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
                if total_data:
                    prices = [p['total_price'] for p in total_data]
                    logger.info(f"  30åˆ†åŒºåˆ‡ã‚Šä¾¡æ ¼ç¯„å›²: {min(prices):,} - {max(prices):,} NESO")
            else:
                results[interval] = {
                    'total_buckets': 0,
                    'saved': False,
                    'latest_total': 0,
                    'latest_count': 0,
                    'time_range': "N/A"
                }
        
        return results
    
    def debug_30min_bucketing(self, interval, max_items=2):
        """30åˆ†åŒºåˆ‡ã‚Šã®ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º"""
        history_file = os.path.join(self.history_dir, f"history_{interval}.json")
        
        if not os.path.exists(history_file):
            return
        
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"30åˆ†åŒºåˆ‡ã‚Šãƒ‡ãƒãƒƒã‚° {interval}:")
            
            for i, (item_id, item_history) in enumerate(data.items()):
                if i >= max_items:
                    break
                
                logger.info(f"  ã‚¢ã‚¤ãƒ†ãƒ  {item_id}:")
                logger.info(f"    å…ƒãƒ‡ãƒ¼ã‚¿æ•°: {len(item_history)}")
                
                # 30åˆ†åŒºåˆ‡ã‚Šå¹³å‡åŒ–ãƒ†ã‚¹ãƒˆ
                averaged = self.aggregate_prices_per_30min(item_history)
                logger.info(f"    30åˆ†åŒºåˆ‡ã‚Šå¾Œ: {len(averaged)}æ™‚é–“æ ")
                
                for bucket in averaged[:3]:  # æœ€åˆã®3æ™‚é–“æ ã‚’è¡¨ç¤º
                    logger.info(f"      {bucket['timestamp']}: {bucket['price']:,} NESO (å…ƒ{bucket['original_count']}ãƒã‚¤ãƒ³ãƒˆ)")
                        
        except Exception as e:
            logger.error(f"30åˆ†åŒºåˆ‡ã‚Šãƒ‡ãƒãƒƒã‚°ã‚¨ãƒ©ãƒ¼ {interval}: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼š30åˆ†åŒºåˆ‡ã‚Šå¹³å‡åŒ–ç·ä¾¡æ ¼é›†è¨ˆ"""
    logger.info("=" * 50)
    logger.info("MapleStoryç·ä¾¡æ ¼é›†è¨ˆï¼ˆ30åˆ†åŒºåˆ‡ã‚Šå¹³å‡åŒ–ç‰ˆï¼‰é–‹å§‹")
    logger.info("=" * 50)
    
    try:
        aggregator = TimestampTotalAggregator()
        
        # 30åˆ†åŒºåˆ‡ã‚Šãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
        logger.info("ğŸ• 30åˆ†åŒºåˆ‡ã‚Šãƒ‡ãƒãƒƒã‚°:")
        for interval in aggregator.intervals:
            aggregator.debug_30min_bucketing(interval)
        
        # é›†è¨ˆå‡¦ç†å®Ÿè¡Œ
        results = aggregator.process_all()
        
        # çµæœè¡¨ç¤º
        logger.info("ğŸ“Š 30åˆ†åŒºåˆ‡ã‚Šé›†è¨ˆçµæœ:")
        for interval, result in results.items():
            logger.info(f"  {interval}: {result['total_buckets']}æ™‚é–“æ ")
            logger.info(f"    æ™‚é–“ç¯„å›²: {result['time_range']}")
            if result['latest_total'] > 0:
                logger.info(f"    æœ€æ–°ç·ä¾¡æ ¼: {result['latest_total']:,} NESO ({result['latest_count']}ã‚¢ã‚¤ãƒ†ãƒ )")
        
        successful = sum(1 for r in results.values() if r['saved'])
        logger.info(f"âœ… 30åˆ†åŒºåˆ‡ã‚Šå‡¦ç†å®Œäº†: {successful}/{len(aggregator.intervals)}é–“éš”æˆåŠŸ")
        
        return successful
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

if __name__ == "__main__":
    main()
