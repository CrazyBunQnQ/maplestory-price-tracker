#!/usr/bin/env python3
import json
import time
import os
from datetime import datetime, timedelta
from collections import deque
import logging
import statistics

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TotalPriceAggregator:
    def __init__(self, json_file_path="data/equipment_prices.json", 
                 history_dir="data/price_history"):
        self.json_file_path = json_file_path
        self.history_dir = history_dir
        
        # å¼·åˆ¶ãƒ‡ãƒ¼ã‚¿ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥è¨­å®š
        self.force_data_refresh = os.getenv('FORCE_DATA_REFRESH', 'true').lower() == 'true'
        self.force_rebuild_aggregation = os.getenv('FORCE_REBUILD_AGGREGATION', 'false').lower() == 'true'
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(history_dir, exist_ok=True)
        
        # ãƒãƒ£ãƒ¼ãƒˆç”¨æ™‚é–“é–“éš”è¨­å®šï¼ˆ30åˆ†æ¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é›†ç´„ï¼‰
        self.price_intervals = {
            '1hour': {
                'interval': timedelta(hours=1),
                'maxlen': 168,  # 1é€±é–“åˆ†ï¼ˆ168æ™‚é–“ï¼‰
                'description': '1é€±é–“åˆ†ï¼ˆ1æ™‚é–“æ¯ï¼‰'
            },
            '12hour': {
                'interval': timedelta(hours=12),
                'maxlen': 60,   # 1ãƒ¶æœˆåˆ†ï¼ˆ60å› = 30æ—¥ï¼‰
                'description': '1ãƒ¶æœˆåˆ†ï¼ˆ12æ™‚é–“æ¯ï¼‰'
            },
            '1day': {
                'interval': timedelta(days=1),
                'maxlen': 365,  # 1å¹´åˆ†ï¼ˆ365æ—¥ï¼‰
                'description': '1å¹´åˆ†ï¼ˆ1æ—¥æ¯ï¼‰'
            }
        }
        
        # 30åˆ†æ¯ã®ç·ä¾¡æ ¼ç”Ÿãƒ‡ãƒ¼ã‚¿
        self.total_price_raw_data = deque(maxlen=2880)  # 30æ—¥åˆ†ã®30åˆ†æ¯ãƒ‡ãƒ¼ã‚¿
        
        # é›†ç´„æ¸ˆã¿ç·ä¾¡æ ¼å±¥æ­´
        self.total_price_history = {}
        
        logger.info("ğŸ”§ ç·ä¾¡æ ¼é›†ç´„ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆ30åˆ†æ¯ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰")
        logger.info(f"ğŸ”„ å¼·åˆ¶ãƒ‡ãƒ¼ã‚¿ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥: {'æœ‰åŠ¹' if self.force_data_refresh else 'ç„¡åŠ¹'}")
        logger.info(f"ğŸ—ï¸ å¼·åˆ¶ãƒªãƒ“ãƒ«ãƒ‰: {'æœ‰åŠ¹' if self.force_rebuild_aggregation else 'ç„¡åŠ¹'}")
        
        self.load_existing_data()

    def load_existing_data(self):
        """æ—¢å­˜ã®ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            # 30åˆ†æ¯ã®ç·ä¾¡æ ¼ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            total_raw_file = os.path.join(self.history_dir, "total_price_raw_data.json")
            if os.path.exists(total_raw_file) and not self.force_rebuild_aggregation:
                with open(total_raw_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.total_price_raw_data = deque(data, maxlen=2880)
                    logger.info(f"ç·ä¾¡æ ¼30åˆ†æ¯ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(self.total_price_raw_data)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
            else:
                logger.info("ç·ä¾¡æ ¼30åˆ†æ¯ãƒ‡ãƒ¼ã‚¿: æ–°è¦ä½œæˆã¾ãŸã¯å†æ§‹ç¯‰")
            
            # é›†ç´„æ¸ˆã¿ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            for interval_type in self.price_intervals:
                total_file = os.path.join(self.history_dir, f"total_price_{interval_type}.json")
                if os.path.exists(total_file) and not self.force_rebuild_aggregation:
                    with open(total_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        self.total_price_history[interval_type] = data
                        logger.info(f"ç·ä¾¡æ ¼{interval_type}ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
                else:
                    logger.info(f"ç·ä¾¡æ ¼{interval_type}ãƒ‡ãƒ¼ã‚¿: æ–°è¦ä½œæˆã¾ãŸã¯å†æ§‹ç¯‰")
                        
        except Exception as e:
            logger.warning(f"ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    def collect_current_total_price(self):
        """ç¾åœ¨ã®ç·ä¾¡æ ¼ã‚’åé›†ã—ã¦30åˆ†æ¯ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ """
        try:
            if not os.path.exists(self.json_file_path):
                logger.error(f"ä¾¡æ ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.json_file_path}")
                return False
            
            with open(self.json_file_path, 'r', encoding='utf-8') as f:
                current_data = json.load(f)
            
            # æœ‰åŠ¹ãªä¾¡æ ¼ã‚’åé›†
            valid_prices = []
            for item_id, item_data in current_data.items():
                if not item_data or not isinstance(item_data, dict):
                    continue
                    
                if not item_data.get('item_price'):
                    continue
                
                price_str = str(item_data['item_price']).replace(',', '').replace(' NESO', '').strip()
                try:
                    current_price = int(price_str)
                    if current_price > 0:
                        valid_prices.append(current_price)
                except (ValueError, TypeError):
                    continue
            
            if not valid_prices:
                logger.warning("æœ‰åŠ¹ãªä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            # ç·ä¾¡æ ¼æƒ…å ±ã‚’è¨ˆç®—
            total_price = sum(valid_prices)
            average_price = int(statistics.mean(valid_prices))
            median_price = int(statistics.median(valid_prices))
            min_price = min(valid_prices)
            max_price = max(valid_prices)
            
            timestamp = datetime.now().isoformat()
            
            # 30åˆ†æ¯ã®ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ
            total_point = {
                'timestamp': timestamp,
                'total_price': total_price,
                'average_price': average_price,
                'median_price': median_price,
                'min_price': min_price,
                'max_price': max_price,
                'item_count': len(valid_prices)
            }
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜åˆ†ã®é‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
            current_minute = datetime.now().replace(second=0, microsecond=0)
            
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãŒåŒã˜åˆ†ã®å ´åˆã¯æ›´æ–°ã€ãã†ã§ãªã‘ã‚Œã°è¿½åŠ 
            if (self.total_price_raw_data and 
                len(self.total_price_raw_data) > 0):
                
                last_point = self.total_price_raw_data[-1]
                try:
                    last_time = datetime.fromisoformat(last_point['timestamp'].replace('Z', '+00:00'))
                    last_minute = last_time.replace(second=0, microsecond=0)
                    
                    if current_minute == last_minute:
                        # åŒã˜åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                        self.total_price_raw_data[-1] = total_point
                        logger.info(f"ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ›´æ–°ï¼ˆåŒåˆ†å†…ï¼‰: åˆè¨ˆ{total_price:,} NESO")
                    else:
                        # æ–°ã—ã„åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                        self.total_price_raw_data.append(total_point)
                        logger.info(f"ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿è¿½åŠ : åˆè¨ˆ{total_price:,} NESO, å¹³å‡{average_price:,} NESO ({len(valid_prices)}ã‚¢ã‚¤ãƒ†ãƒ )")
                except Exception:
                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è§£æã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è¿½åŠ 
                    self.total_price_raw_data.append(total_point)
            else:
                # åˆå›ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ç©ºã®å ´åˆ
                self.total_price_raw_data.append(total_point)
                logger.info(f"ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿åˆå›è¿½åŠ : åˆè¨ˆ{total_price:,} NESO, å¹³å‡{average_price:,} NESO ({len(valid_prices)}ã‚¢ã‚¤ãƒ†ãƒ )")
            
            return True
            
        except Exception as e:
            logger.error(f"ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def aggregate_total_price_for_interval(self, interval_type):
        """ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šé–“éš”ã§é›†ç´„"""
        if not self.total_price_raw_data:
            logger.warning(f"30åˆ†æ¯ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³: {interval_type}")
            return None
        
        config = self.price_intervals[interval_type]
        interval_duration = config['interval']
        
        raw_data = list(self.total_price_raw_data)
        aggregated_data = []
        current_group = []
        group_start_time = None
        
        for data_point in raw_data:
            try:
                point_time = datetime.fromisoformat(data_point['timestamp'].replace('Z', '+00:00'))
                
                if group_start_time is None:
                    group_start_time = point_time
                    current_group = [data_point]
                elif point_time - group_start_time < interval_duration:
                    current_group.append(data_point)
                else:
                    # ç¾åœ¨ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é›†ç´„
                    if current_group:
                        aggregated_point = self.create_aggregated_point(current_group)
                        aggregated_data.append(aggregated_point)
                    
                    # æ–°ã—ã„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹
                    group_start_time = point_time
                    current_group = [data_point]
                    
            except Exception as e:
                logger.debug(f"ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # æœ€å¾Œã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å‡¦ç†
        if current_group:
            aggregated_point = self.create_aggregated_point(current_group)
            aggregated_data.append(aggregated_point)
        
        # Chart.jsç”¨ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§è¿”ã™
        return self.format_total_price_chart_data(aggregated_data, interval_type)

    def create_aggregated_point(self, group):
        """ãƒ‡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰é›†ç´„ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ"""
        if not group:
            return None
        
        # å„æŒ‡æ¨™ã®å¹³å‡ã‚’è¨ˆç®—
        avg_total = int(statistics.mean([p['total_price'] for p in group]))
        avg_average = int(statistics.mean([p['average_price'] for p in group]))
        avg_median = int(statistics.mean([p['median_price'] for p in group]))
        min_of_mins = min([p['min_price'] for p in group])
        max_of_maxs = max([p['max_price'] for p in group])
        avg_count = int(statistics.mean([p['item_count'] for p in group]))
        
        return {
            'timestamp': group[-1]['timestamp'],  # æœ€æ–°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä½¿ç”¨
            'total_price': avg_total,
            'average_price': avg_average,
            'median_price': avg_median,
            'min_price': min_of_mins,
            'max_price': max_of_maxs,
            'item_count': avg_count,
            'data_points': len(group)
        }

    def format_total_price_chart_data(self, aggregated_data, interval_type):
        """ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’Chart.jså½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not aggregated_data:
            return {"labels": [], "datasets": []}
        
        # æ™‚åˆ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’é–“éš”ã«å¿œã˜ã¦èª¿æ•´
        def format_time(timestamp_str):
            try:
                timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                if interval_type == '1hour':
                    return timestamp.strftime('%m/%d %H:%M')
                elif interval_type == '12hour':
                    return timestamp.strftime('%m/%d %H:%M')
                else:  # 1day
                    return timestamp.strftime('%m/%d')
            except:
                return timestamp_str
        
        labels = [format_time(point['timestamp']) for point in aggregated_data]
        total_prices = [point['total_price'] for point in aggregated_data]
        average_prices = [point['average_price'] for point in aggregated_data]
        
        config = self.price_intervals[interval_type]
        
        return {
            'labels': labels,
            'datasets': [
                {
                    'label': f'ç·ä¾¡æ ¼ ({config["description"]})',
                    'data': total_prices,
                    'borderColor': '#e74c3c',
                    'backgroundColor': 'rgba(231, 76, 60, 0.1)',
                    'borderWidth': 3,
                    'fill': True,
                    'tension': 0.3,
                    'yAxisID': 'y'
                },
                {
                    'label': f'å¹³å‡ä¾¡æ ¼ ({config["description"]})',
                    'data': average_prices,
                    'borderColor': '#3498db',
                    'backgroundColor': 'rgba(52, 152, 219, 0.1)',
                    'borderWidth': 2,
                    'fill': False,
                    'tension': 0.3,
                    'yAxisID': 'y1'
                }
            ]
        }

    def save_total_price_data(self):
        """ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            # 30åˆ†æ¯ã®ç·ä¾¡æ ¼ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            total_raw_file = os.path.join(self.history_dir, "total_price_raw_data.json")
            with open(total_raw_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.total_price_raw_data), f, ensure_ascii=False, indent=2)
            
            logger.info(f"ç·ä¾¡æ ¼30åˆ†æ¯ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {len(self.total_price_raw_data)}ãƒã‚¤ãƒ³ãƒˆ")
            
            # å„é–“éš”ã®é›†ç´„æ¸ˆã¿ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            for interval_type in self.price_intervals:
                if interval_type in self.total_price_history:
                    total_file = os.path.join(self.history_dir, f"total_price_{interval_type}.json")
                    with open(total_file, 'w', encoding='utf-8') as f:
                        json.dump(self.total_price_history[interval_type], f, ensure_ascii=False, indent=2)
                    
                    dataset_count = len(self.total_price_history[interval_type].get('datasets', []))
                    label_count = len(self.total_price_history[interval_type].get('labels', []))
                    
                    logger.info(f"ç·ä¾¡æ ¼{interval_type}ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ä¿å­˜: {label_count}ãƒã‚¤ãƒ³ãƒˆ, {dataset_count}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
            
        except Exception as e:
            logger.error(f"ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def update_all_aggregations(self):
        """å…¨ã¦ã®é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        try:
            # ç¾åœ¨ã®ç·ä¾¡æ ¼ã‚’åé›†
            if not self.collect_current_total_price():
                logger.error("ç·ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # å„é–“éš”ã§ã®é›†ç´„ã‚’å®Ÿè¡Œ
            updated_intervals = []
            for interval_type in self.price_intervals:
                chart_data = self.aggregate_total_price_for_interval(interval_type)
                if chart_data:
                    self.total_price_history[interval_type] = chart_data
                    updated_intervals.append(interval_type)
                    
                    # é›†ç´„çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›
                    label_count = len(chart_data.get('labels', []))
                    dataset_count = len(chart_data.get('datasets', []))
                    
                    logger.info(f"ç·ä¾¡æ ¼{interval_type}é›†ç´„å®Œäº†: {label_count}ãƒã‚¤ãƒ³ãƒˆ, {dataset_count}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
            
            if updated_intervals:
                self.save_total_price_data()
                logger.info(f"âœ… ç·ä¾¡æ ¼é›†ç´„æ›´æ–°å®Œäº†: {updated_intervals}")
                return True
            else:
                logger.warning("ç·ä¾¡æ ¼é›†ç´„ãƒ‡ãƒ¼ã‚¿ãŒæ›´æ–°ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return False
                
        except Exception as e:
            logger.error(f"ç·ä¾¡æ ¼é›†ç´„æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def get_statistics(self):
        """ç·ä¾¡æ ¼é›†ç´„çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        stats = {
            'raw_data_points': len(self.total_price_raw_data),
            'intervals': {},
            'configuration': {
                'force_data_refresh': self.force_data_refresh,
                'force_rebuild_aggregation': self.force_rebuild_aggregation,
                'data_collection_interval': '30åˆ†æ¯'
            }
        }
        
        for interval_type, config in self.price_intervals.items():
            if interval_type in self.total_price_history:
                chart_data = self.total_price_history[interval_type]
                label_count = len(chart_data.get('labels', []))
                dataset_count = len(chart_data.get('datasets', []))
                
                stats['intervals'][interval_type] = {
                    'chart_points': label_count,
                    'datasets': dataset_count,
                    'description': config['description'],
                    'max_points': config['maxlen'],
                    'has_data': label_count > 0
                }
            else:
                stats['intervals'][interval_type] = {
                    'chart_points': 0,
                    'datasets': 0,
                    'description': config['description'],
                    'max_points': config['maxlen'],
                    'has_data': False
                }
        
        return stats

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼šç·ä¾¡æ ¼é›†ç´„å‡¦ç†"""
    logger.info("=" * 50)
    logger.info("MapleStoryç·ä¾¡æ ¼é›†ç´„å‡¦ç†é–‹å§‹ï¼ˆ30åˆ†æ¯ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰")
    logger.info("=" * 50)
    
    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        logger.info("ç·ä¾¡æ ¼é›†ç´„ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: data/price_history")
        aggregator = TotalPriceAggregator()
        
        # å…¨é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
        success = aggregator.update_all_aggregations()
        
        # çµ±è¨ˆè¡¨ç¤º
        stats = aggregator.get_statistics()
        logger.info(f"ğŸ“Š ç·ä¾¡æ ¼é›†ç´„çµ±è¨ˆ:")
        logger.info(f"  30åˆ†æ¯ç”Ÿãƒ‡ãƒ¼ã‚¿: {stats['raw_data_points']}ãƒã‚¤ãƒ³ãƒˆ")
        logger.info(f"  è¨­å®š: å¼·åˆ¶ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥={stats['configuration']['force_data_refresh']}, å¼·åˆ¶ãƒªãƒ“ãƒ«ãƒ‰={stats['configuration']['force_rebuild_aggregation']}")
        
        for interval, data in stats['intervals'].items():
            status = "âœ…" if data['has_data'] else "âŒ"
            logger.info(f"  {interval}: {status} {data['chart_points']}ãƒã‚¤ãƒ³ãƒˆ ({data['description']}) - {data['datasets']}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
        
        logger.info("=" * 50)
        if success:
            logger.info(f"âœ… ç·ä¾¡æ ¼é›†ç´„å‡¦ç†å®Œäº†")
        else:
            logger.info(f"âš ï¸ ç·ä¾¡æ ¼é›†ç´„å‡¦ç†ã§å•é¡ŒãŒç™ºç”Ÿ")
        logger.info("=" * 50)
        
        return success
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        raise

if __name__ == "__main__":
    main()
